{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagined Ecology\n",
    "## Jake Bauer, A15188631"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1302281 characters\n",
      "81 unique characters\n",
      "1302281\n",
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '\"' :   2,\n",
      "  '#' :   3,\n",
      "  '%' :   4,\n",
      "  '&' :   5,\n",
      "  \"'\" :   6,\n",
      "  '(' :   7,\n",
      "  ')' :   8,\n",
      "  '+' :   9,\n",
      "  ',' :  10,\n",
      "  '-' :  11,\n",
      "  '.' :  12,\n",
      "  '/' :  13,\n",
      "  '0' :  14,\n",
      "  '1' :  15,\n",
      "  '2' :  16,\n",
      "  '3' :  17,\n",
      "  '4' :  18,\n",
      "  '5' :  19,\n",
      "  ...\n",
      "}\n",
      "'\\ufeff\"\\n\\n\\n\\n\\nPinus ' ---- characters mapped to int ---- > [80  2  0  0  0  0  0 41 60 65 72 70  1]\n",
      "﻿\n",
      "\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'\\ufeff\"\\n\\n\\n\\n\\nPinus rigida\\nPitch Pine, Northern Pitch Pine: \\nHabitat\\n    \\n    \\n      native to Eastern North'\n",
      "' America\\n      cold hardy to zone 4\\n      often found on poor, sandy soil that fails to support other'\n",
      "' species\\n    \\n    \\n    \\n    \\n    Habit\\n      and Form \\n    \\n      evergreen tree\\n      open conical s'\n",
      "\"hape, becoming more irregular with age\\n      typically 40 to 60' wide and 30 to 50' wide \\n      mediu\"\n",
      "'m texture \\n      medium growth rate, slows down as tree reaches mature height \\n    \\n    \\n    \\n      \\n'\n",
      "Input data:  '\\ufeff\"\\n\\n\\n\\n\\nPinus rigida\\nPitch Pine, Northern Pitch Pine: \\nHabitat\\n    \\n    \\n      native to Eastern Nort'\n",
      "Target data: '\"\\n\\n\\n\\n\\nPinus rigida\\nPitch Pine, Northern Pitch Pine: \\nHabitat\\n    \\n    \\n      native to Eastern North'\n",
      "Step    0\n",
      "  input: 80 ('\\ufeff')\n",
      "  expected output: 2 ('\"')\n",
      "Step    1\n",
      "  input: 2 ('\"')\n",
      "  expected output: 0 ('\\n')\n",
      "Step    2\n",
      "  input: 0 ('\\n')\n",
      "  expected output: 0 ('\\n')\n",
      "Step    3\n",
      "  input: 0 ('\\n')\n",
      "  expected output: 0 ('\\n')\n",
      "Step    4\n",
      "  input: 0 ('\\n')\n",
      "  expected output: 0 ('\\n')\n",
      "(64, 100, 81) # (batch_size, sequence_length, vocab_size)\n",
      "Input: \n",
      " 'e\\n    \\n      for edible fruit\\n      to attract wildlife\\n      excellent for fall color\\n      useful '\n",
      "\n",
      "Next Char Predictions: \n",
      " 'qK-\\nuwIBoS-f;CU\"2/\\n6t0DRa7uD2ceeXBc¨efmriª#&bglpriLªAqAP3Yc:IatDTfw(Uwk+\".d1Ona86e1n)87:Hq\";XGCiC7jX'\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "#reading training data\n",
    "file = open('plant-data-final-encoded.txt')\n",
    "text = file.read()\n",
    "\n",
    "#reading names for generation\n",
    "import csv\n",
    "names = []\n",
    "with open(\"the-plant-list.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile) \n",
    "    for row in reader: # each row is a list\n",
    "        names.append(row)\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "print(len(text))\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')\n",
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n",
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "    \n",
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n",
    "    \n",
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 1256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024#512\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "    \n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    \n",
    "sampled_indices = tf.random.multinomial(example_batch_predictions[0], num_samples=1) \n",
    "#sample_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 81)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.38514\n",
      "Epoch 1/35\n",
      "203/203 [==============================] - 22s 108ms/step - loss: 2.0450\n",
      "Epoch 2/35\n",
      "203/203 [==============================] - 19s 93ms/step - loss: 1.1473\n",
      "Epoch 3/35\n",
      "203/203 [==============================] - 20s 100ms/step - loss: 0.9340\n",
      "Epoch 4/35\n",
      "203/203 [==============================] - 18s 89ms/step - loss: 0.8412\n",
      "Epoch 5/35\n",
      "203/203 [==============================] - 19s 94ms/step - loss: 0.7831\n",
      "Epoch 6/35\n",
      "203/203 [==============================] - 19s 94ms/step - loss: 0.7392\n",
      "Epoch 7/35\n",
      "203/203 [==============================] - 20s 96ms/step - loss: 0.7027\n",
      "Epoch 8/35\n",
      "203/203 [==============================] - 21s 102ms/step - loss: 0.6716\n",
      "Epoch 9/35\n",
      "203/203 [==============================] - 20s 100ms/step - loss: 0.6439\n",
      "Epoch 10/35\n",
      "203/203 [==============================] - 19s 94ms/step - loss: 0.6166\n",
      "Epoch 11/35\n",
      "203/203 [==============================] - 19s 95ms/step - loss: 0.5938\n",
      "Epoch 12/35\n",
      "203/203 [==============================] - 18s 89ms/step - loss: 0.5694\n",
      "Epoch 13/35\n",
      "203/203 [==============================] - 18s 88ms/step - loss: 0.5500\n",
      "Epoch 14/35\n",
      "203/203 [==============================] - 18s 91ms/step - loss: 0.5305\n",
      "Epoch 15/35\n",
      "203/203 [==============================] - 18s 86ms/step - loss: 0.5133\n",
      "Epoch 16/35\n",
      "203/203 [==============================] - 18s 88ms/step - loss: 0.4976\n",
      "Epoch 17/35\n",
      "203/203 [==============================] - 18s 87ms/step - loss: 0.4827\n",
      "Epoch 18/35\n",
      "203/203 [==============================] - 18s 91ms/step - loss: 0.4667\n",
      "Epoch 19/35\n",
      "203/203 [==============================] - 18s 90ms/step - loss: 0.4581\n",
      "Epoch 20/35\n",
      "203/203 [==============================] - 19s 92ms/step - loss: 0.4451\n",
      "Epoch 21/35\n",
      "203/203 [==============================] - 18s 87ms/step - loss: 0.4360\n",
      "Epoch 22/35\n",
      "203/203 [==============================] - 17s 86ms/step - loss: 0.4285\n",
      "Epoch 23/35\n",
      "203/203 [==============================] - 18s 87ms/step - loss: 0.4214\n",
      "Epoch 24/35\n",
      "203/203 [==============================] - 19s 93ms/step - loss: 0.4158\n",
      "Epoch 25/35\n",
      "203/203 [==============================] - 18s 88ms/step - loss: 0.4111\n",
      "Epoch 26/35\n",
      "203/203 [==============================] - 18s 88ms/step - loss: 0.4060\n",
      "Epoch 27/35\n",
      "203/203 [==============================] - 18s 88ms/step - loss: 0.4021\n",
      "Epoch 28/35\n",
      "203/203 [==============================] - 18s 87ms/step - loss: 0.3973\n",
      "Epoch 29/35\n",
      "203/203 [==============================] - 19s 93ms/step - loss: 0.3959\n",
      "Epoch 30/35\n",
      "203/203 [==============================] - 18s 88ms/step - loss: 0.3955\n",
      "Epoch 31/35\n",
      "203/203 [==============================] - 18s 91ms/step - loss: 0.3931\n",
      "Epoch 32/35\n",
      "203/203 [==============================] - 19s 92ms/step - loss: 0.3925\n",
      "Epoch 33/35\n",
      "203/203 [==============================] - 18s 87ms/step - loss: 0.3906\n",
      "Epoch 34/35\n",
      "203/203 [==============================] - 20s 97ms/step - loss: 0.3896\n",
      "Epoch 35/35\n",
      "203/203 [==============================] - 18s 87ms/step - loss: 0.3888\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "#   return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "EPOCHS=35\n",
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = .4\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n",
    "\n",
    "import random\n",
    "for i in range(100):\n",
    "    r = random.randint(1,23830)\n",
    "    plant_name = names[r][2]\n",
    "    plants = generate_text(model, start_string=plant_name);\n",
    "    output_plants_text = open('100-generated-plants.txt','a');\n",
    "    output_plants_text.write(\"\\n\\n--------------------------------------------------------------\\n\\n\")\n",
    "    output_plants_text.write(plants);\n",
    "    #print(plants);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
